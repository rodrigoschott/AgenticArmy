# üî¨ PESQUISA DE MODELOS RECOMENDADOS PARA CREWAI
## An√°lise Completa de Modelos para Tool Calling e Workflows Agentic

**Data da Pesquisa**: 31 de Outubro de 2025  
**Contexto**: Busca por alternativas ap√≥s descobrir incompatibilidade do gpt-oss com tool calling no CrewAI

---

## üìä FONTES DE REFER√äNCIA

### Principais Leaderboards e Estudos
1. **Berkeley Function Calling Leaderboard (BFCL) V4** (Atualizado: Agosto 2025)
   - URL: https://gorilla.cs.berkeley.edu/leaderboard.html
   - Refer√™ncia acad√™mica mais respeitada para avalia√ß√£o de function calling
   - Metodologia: AST (Abstract Syntax Tree) para avalia√ß√£o precisa
   - M√©tricas: Multi-turn interactions, enterprise functions, agentic evaluation

2. **Docker Local LLM Tool Calling Evaluation** (Junho 2025)
   - URL: https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/
   - Testes pr√°ticos com 21 modelos e 3,570 casos de teste
   - Hardware: MacBook Pro M4 Max, 128GB RAM
   - M√©tricas: Tool Invocation, Tool Selection, Parameter Accuracy

3. **Collabnix Ollama Models Guide** (Agosto 2025)
   - URL: https://collabnix.com/best-ollama-models-for-function-calling-tools-complete-guide-2025/
   - Foco em modelos Ollama com suporte nativo a tool calling
   - Compara√ß√£o de requisitos de hardware e performance

4. **CrewAI Community Forums**
   - URL: https://community.crewai.com/
   - Experi√™ncias pr√°ticas de desenvolvedores com diferentes modelos
   - Recomenda√ß√µes espec√≠ficas para Qwen 2.5 e Qwen 3

---

## üèÜ TOP 10 MODELOS PARA TOOL CALLING (2025)

### Ranking Consolidado (por F1 Score em Tool Selection)

| Posi√ß√£o | Modelo | F1 Score | Lat√™ncia | Tamanho | Dispon√≠vel no Ollama | Status |
|---------|---------|----------|----------|---------|----------------------|--------|
| ü•á 1 | **GPT-4** | 0.974 | ~5s | - | ‚ùå | Hosted (OpenAI) |
| ü•à 2 | **Qwen 3 (14B)** | 0.971 | ~142s | 14B | ‚úÖ | **RECOMENDADO** |
| ü•â 3 | **Qwen 3 (14B Q6_K)** | 0.943 | ~120s | 9GB | ‚úÖ | **RECOMENDADO** |
| 4 | **Claude 3 Haiku** | 0.933 | ~3s | - | ‚ùå | Hosted (Anthropic) |
| 5 | **Qwen 3 (8B)** | 0.933 | ~84s | 8B | ‚úÖ | **RECOMENDADO** |
| 6 | **Qwen 3 (8B Q4_K_M)** | 0.919 | ~70s | 5GB | ‚úÖ | **RECOMENDADO** |
| 7 | **GPT-3.5 Turbo** | 0.899 | ~3s | - | ‚ùå | Hosted (OpenAI) |
| 8 | **GPT-4o Mini** | 0.852 | ~2s | - | ‚ùå | Hosted (OpenAI) |
| 9 | **Llama 3.1 (8B)** | 0.835 | ~90s | 8B | ‚úÖ | Alternativa |
| 10 | **Qwen 2.5 (14B Q4_K_M)** | 0.812 | ~130s | 9GB | ‚úÖ | J√° testado ‚úÖ |

---

## üéØ RECOMENDA√á√ïES ESPEC√çFICAS PARA NOSSO CEN√ÅRIO

### Baseado em:
- ‚úÖ Compatibilidade com CrewAI (testado com qwen2.5:14b)
- ‚úÖ Disponibilidade no Ollama (modelos locais)
- ‚úÖ Suporte a tool calling robusto
- ‚úÖ Performance em workflows complexos
- ‚úÖ Suporte ao idioma portugu√™s (importante para nossos workflows)

---

## üåü TIER S: ALTAMENTE RECOMENDADOS

### 1. **Qwen 3 (14B) - NOVA GERA√á√ÉO** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
# Instala√ß√£o
ollama pull qwen3:14b

# Vers√£o quantizada (menor uso de RAM)
ollama pull qwen3:14b-q4_k_m
```

**Especifica√ß√µes:**
- **F1 Score**: 0.971 (praticamente empatado com GPT-4!)
- **Tamanho**: 14B par√¢metros (~9GB quantizado)
- **Lat√™ncia**: 120-142s (aceit√°vel para workflows complexos)
- **RAM Necess√°ria**: 16GB+ recomendado

**Por que escolher:**
- ‚úÖ **Melhor modelo local** para tool calling segundo Docker evaluation
- ‚úÖ **Evolu√ß√£o do Qwen 2.5** que j√° testamos e funciona bem
- ‚úÖ **Acur√°cia excepcional** em schema understanding (96%)
- ‚úÖ **Multilingual** - excelente suporte a portugu√™s
- ‚úÖ **Comunidade CrewAI** confirma compatibilidade perfeita
- ‚úÖ **Reasoning avan√ßado** - melhor que Qwen 2.5 em racioc√≠nio complexo

**Pontos de aten√ß√£o:**
- ‚ö†Ô∏è Lat√™ncia maior que modelos menores (trade-off por qualidade)
- ‚ö†Ô∏è Requer hardware razo√°vel (16GB+ RAM)

**Cen√°rios ideais:**
- Workflows com m√∫ltiplas ferramentas
- Tarefas que exigem racioc√≠nio complexo
- Projetos que priorizam acur√°cia sobre velocidade
- Produ√ß√£o (maior confiabilidade)

---

### 2. **Qwen 3 (8B) - MELHOR EQUIL√çBRIO** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
# Instala√ß√£o
ollama pull qwen3:8b

# Vers√£o quantizada
ollama pull qwen3:8b-q4_k_m
```

**Especifica√ß√µes:**
- **F1 Score**: 0.933 (empata com Claude 3 Haiku!)
- **Tamanho**: 8B par√¢metros (~5GB quantizado)
- **Lat√™ncia**: 70-84s (quase 50% mais r√°pido que 14B)
- **RAM Necess√°ria**: 8GB+ recomendado

**Por que escolher:**
- ‚úÖ **Melhor custo-benef√≠cio** entre performance e velocidade
- ‚úÖ **F1 Score excelente** (0.933) - perde pouco para o 14B
- ‚úÖ **Lat√™ncia reduzida** - metade do tempo do 14B
- ‚úÖ **Menor uso de RAM** - roda em hardware mais modesto
- ‚úÖ **Mesma arquitetura** do Qwen 3 14B
- ‚úÖ **Ideal para desenvolvimento** iterativo

**Pontos de aten√ß√£o:**
- ‚ö†Ô∏è Pode ter dificuldade em cen√°rios extremamente complexos
- ‚ö†Ô∏è Reasoning um pouco inferior ao 14B

**Cen√°rios ideais:**
- Desenvolvimento e testes
- Workflows com complexidade m√©dia
- Hardware limitado (8-16GB RAM)
- Aplica√ß√µes que precisam de respostas mais r√°pidas

---

### 3. **Qwen 2.5 (14B) - J√Å VALIDADO** ‚≠ê‚≠ê‚≠ê‚≠ê
```bash
# J√° instalado
ollama pull qwen2.5:14b
```

**Especifica√ß√µes:**
- **F1 Score**: 0.812
- **Tamanho**: 14B par√¢metros (~9GB)
- **Lat√™ncia**: ~130s
- **RAM Necess√°ria**: 16GB+ recomendado

**Por que escolher:**
- ‚úÖ **J√° testado e validado** no nosso projeto ‚úÖ
- ‚úÖ **Funciona perfeitamente** com CrewAI (teste confirmou)
- ‚úÖ **Documenta√ß√£o ampla** - conhecemos seu comportamento
- ‚úÖ **Est√°vel** - vers√£o madura e confi√°vel
- ‚úÖ **Multilingual** - bom suporte a portugu√™s

**Pontos de aten√ß√£o:**
- ‚ö†Ô∏è Qwen 3 √© superior em performance (0.971 vs 0.812)
- ‚ö†Ô∏è Vers√£o anterior - Qwen 3 tem melhorias significativas

**Cen√°rios ideais:**
- Continuar com modelo j√° validado
- Evitar mudan√ßas disruptivas no projeto
- Manter estabilidade no curto prazo

**üí° Recomenda√ß√£o:** Migrar para Qwen 3 quando poss√≠vel para ganhar +16% em acur√°cia

---

## üéñÔ∏è TIER A: ALTERNATIVAS S√ìLIDAS

### 4. **Llama 3.1 (8B Instruct)** ‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull llama3.1:8b-instruct
```

**Especifica√ß√µes:**
- **F1 Score**: 0.835
- **Tamanho**: 8B par√¢metros
- **Lat√™ncia**: ~90s
- **RAM Necess√°ria**: 8GB+

**Por que considerar:**
- ‚úÖ **Meta oficial** - suporte corporativo forte
- ‚úÖ **Amplamente testado** pela comunidade
- ‚úÖ **Boa documenta√ß√£o** e exemplos
- ‚úÖ **Confi√°vel** para tool calling

**Limita√ß√µes:**
- ‚ö†Ô∏è F1 Score inferior aos Qwen (0.835 vs 0.933/0.971)
- ‚ö†Ô∏è Suporte multilingual n√£o √© o forte

**Cen√°rios ideais:**
- Integra√ß√£o com ecossistema Meta/Llama
- Casos de uso em ingl√™s prioritariamente
- Fallback se Qwen apresentar problemas

---

### 5. **Mistral 7B Instruct v0.3** ‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull mistral:7b-instruct
```

**Especifica√ß√µes:**
- **F1 Score**: 0.85-0.86 (estimado)
- **Tamanho**: 7B par√¢metros
- **Lat√™ncia**: ~80s (mais r√°pido)
- **RAM Necess√°ria**: 7GB+

**Por que considerar:**
- ‚úÖ **Muito eficiente** - menor uso de recursos
- ‚úÖ **R√°pido** - √≥tima lat√™ncia
- ‚úÖ **Multilingual** - bom suporte europeu
- ‚úÖ **JSON schema adherence** - excelente com estruturas

**Limita√ß√µes:**
- ‚ö†Ô∏è Menor contexto que Qwen/Llama
- ‚ö†Ô∏è F1 Score um pouco inferior

**Cen√°rios ideais:**
- Hardware muito limitado
- Aplica√ß√µes que priorizam velocidade
- Tarefas estruturadas (JSON, APIs)

---

### 6. **Llama 3.1 (70B Instruct)** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (POWERHOUSE)
```bash
ollama pull llama3.1:70b-instruct
```

**Especifica√ß√µes:**
- **F1 Score**: 0.94-0.96 (estimado, pr√≥ximo ao GPT-4)
- **Tamanho**: 70B par√¢metros (~40GB)
- **Lat√™ncia**: ~240s (muito lento)
- **RAM Necess√°ria**: 64GB+ recomendado

**Por que considerar:**
- ‚úÖ **Acur√°cia excepcional** - melhor reasoning
- ‚úÖ **Complex multi-step** - excelente em workflows complexos
- ‚úÖ **Error handling** superior
- ‚úÖ **Context understanding** avan√ßado

**Limita√ß√µes:**
- ‚ùå **Requisitos de hardware** muito altos (64GB+ RAM)
- ‚ùå **Lat√™ncia** proibitiva para uso interativo
- ‚ùå **Overkill** para maioria dos casos

**Cen√°rios ideais:**
- Hardware de servidor dispon√≠vel
- Workflows cr√≠ticos que exigem m√°xima acur√°cia
- Produ√ß√£o em grande escala
- N√£o se importa com lat√™ncia (batch processing)

---

## üî¨ TIER B: MODELOS ESPECIALIZADOS

### 7. **CodeLlama 13B Instruct** ‚≠ê‚≠ê‚≠ê
```bash
ollama pull codellama:13b-instruct
```

**Especifica√ß√µes:**
- **F1 Score**: 0.88 (Docker test)
- **Tamanho**: 13B par√¢metros
- **Especializa√ß√£o**: Code generation e debugging

**Por que considerar:**
- ‚úÖ **Especializado em c√≥digo** - excelente para DevOps
- ‚úÖ **API documentation** - entende schemas t√©cnicos
- ‚úÖ **Code generation** superior

**Limita√ß√µes:**
- ‚ö†Ô∏è **N√£o √© generalista** - focado em c√≥digo
- ‚ö†Ô∏è **Portugu√™s limitado**

**Cen√°rios ideais:**
- Workflows que geram c√≥digo
- Integra√ß√£o com APIs t√©cnicas
- DevOps e CI/CD automation

---

### 8. **Mixtral 8x7B Instruct** ‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull mixtral:8x7b-instruct
```

**Especifica√ß√µes:**
- **F1 Score**: 0.88
- **Tamanho**: 8√ó7B par√¢metros (MoE) (~24GB)
- **Arquitetura**: Mixture of Experts

**Por que considerar:**
- ‚úÖ **Versatilidade** - experts para diferentes dom√≠nios
- ‚úÖ **Multilingual** excelente
- ‚úÖ **Complex schemas** - roteamento inteligente

**Limita√ß√µes:**
- ‚ö†Ô∏è **Alto uso de RAM** (24GB+)
- ‚ö†Ô∏è **Lat√™ncia moderada** (~120s)

**Cen√°rios ideais:**
- Aplica√ß√µes multi-dom√≠nio
- Workflows internacionais
- Quando precisa de especializa√ß√£o din√¢mica

---

## ‚ùå MODELOS N√ÉO RECOMENDADOS PARA CREWAI

### üö´ **gpt-oss:latest** - INCOMPAT√çVEL
```bash
# N√ÉO USAR para workflows com tool calling
ollama pull gpt-oss:latest
```

**Por que evitar:**
- ‚ùå **Multi-channel format** incompat√≠vel com CrewAI
- ‚ùå **Falha em workflows** com ferramentas (validado)
- ‚ùå **Resposta None/empty** em contextos complexos
- ‚ö†Ô∏è Funciona apenas em conversas simples

**Status:** ‚úÖ J√° documentado e warning implementado em `crew_paraty.py`

---

### üö´ **xLAM-2-8B-fc-r** - INST√ÅVEL
**Por que evitar:**
- ‚ùå **Eager invocation** - chama tools desnecessariamente
- ‚ùå **Wrong tool selection** frequente
- ‚ùå **Invalid arguments** - par√¢metros malformados

---

### üö´ **watt-tool-8B** - BAIXA PERFORMANCE
**Por que evitar:**
- ‚ùå **F1 Score**: 0.484 (muito baixo)
- ‚ùå **Ignora tool responses** frequentemente
- ‚ùå **Conversas incompletas**

---

## üìã COMPATIBILIDADE COM OLLAMA

### ‚úÖ Modelos Dispon√≠veis no Ollama (Verificados)

| Modelo | Comando | Status | F1 Score |
|--------|---------|--------|----------|
| Qwen 3 (14B) | `ollama pull qwen3:14b` | ‚úÖ Dispon√≠vel | 0.971 |
| Qwen 3 (8B) | `ollama pull qwen3:8b` | ‚úÖ Dispon√≠vel | 0.933 |
| Qwen 2.5 (14B) | `ollama pull qwen2.5:14b` | ‚úÖ Instalado | 0.812 |
| Llama 3.1 (8B) | `ollama pull llama3.1:8b-instruct` | ‚úÖ Dispon√≠vel | 0.835 |
| Llama 3.1 (70B) | `ollama pull llama3.1:70b-instruct` | ‚úÖ Dispon√≠vel | ~0.95 |
| Llama 3.2 | `ollama pull llama3.2` | ‚úÖ Instalado | 0.727 |
| Mistral 7B | `ollama pull mistral:7b-instruct` | ‚úÖ Dispon√≠vel | 0.85-0.86 |
| CodeLlama 13B | `ollama pull codellama:13b-instruct` | ‚úÖ Dispon√≠vel | 0.88 |
| Mixtral 8x7B | `ollama pull mixtral:8x7b-instruct` | ‚úÖ Dispon√≠vel | 0.88 |
| DeepSeek Coder | `ollama pull deepseek-coder:33b` | ‚úÖ Instalado | N√£o testado |
| GLM-4.6 | `ollama pull glm-4.6` | ‚úÖ Instalado | N√£o testado |

---

## üéØ DECIS√ÉO FINAL: NOSSO PLANO DE A√á√ÉO

### üöÄ RECOMENDA√á√ÉO IMEDIATA

**Curto Prazo (Pr√≥ximas 2 semanas):**
1. **Continuar com Qwen 2.5:14b** para estabilidade ‚úÖ
   - J√° validado, funciona perfeitamente
   - Evita riscos de mudan√ßas disruptivas
   - F1 Score 0.812 √© s√≥lido

**M√©dio Prazo (1 m√™s):**
2. **Migrar para Qwen 3:8b ou Qwen 3:14b** üéØ
   - Ganho de 12-16% em acur√°cia (0.933 ou 0.971)
   - Mesma fam√≠lia Qwen = migra√ß√£o suave
   - Testar em ambiente de dev primeiro

**Testes Paralelos:**
3. **Avaliar Llama 3.1:8b-instruct** como fallback
   - Diferente fam√≠lia = comportamento diferente
   - Pode ser √∫til em cen√°rios espec√≠ficos
   - F1 Score 0.835 √© competitivo

---

### üìä MATRIZ DE DECIS√ÉO

| Crit√©rio | Qwen 2.5:14b | Qwen 3:8b | Qwen 3:14b | Llama 3.1:8b |
|----------|--------------|-----------|------------|--------------|
| **F1 Score** | 0.812 ‚≠ê‚≠ê‚≠ê | 0.933 ‚≠ê‚≠ê‚≠ê‚≠ê | 0.971 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0.835 ‚≠ê‚≠ê‚≠ê |
| **Lat√™ncia** | ~130s ‚≠ê‚≠ê‚≠ê | ~70s ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~142s ‚≠ê‚≠ê‚≠ê | ~90s ‚≠ê‚≠ê‚≠ê‚≠ê |
| **RAM** | 16GB ‚≠ê‚≠ê‚≠ê | 8GB ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 16GB ‚≠ê‚≠ê‚≠ê | 8GB ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Portugu√™s** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Validado** | ‚úÖ | ‚è≥ | ‚è≥ | ‚è≥ |
| **Recomenda√ß√£o** | **Manter** | **Testar** | **Migrar** | **Backup** |

---

## üîß PLANO DE MIGRA√á√ÉO

### Fase 1: Prepara√ß√£o (1 dia)
```bash
# Instalar novos modelos
ollama pull qwen3:8b-q4_k_m
ollama pull qwen3:14b-q4_k_m
ollama pull llama3.1:8b-instruct

# Verificar instala√ß√£o
ollama list
```

### Fase 2: Testes (2-3 dias)
```bash
# Rodar testes de compatibilidade
poetry run python test_model_compatibility.py

# Rodar teste avan√ßado com tool calls
poetry run python test_gptoss_toolcalls.py

# Testar workflows reais
poetry run start
# Selecionar Qwen 3:8b e rodar Workflow A, B, C, D
```

### Fase 3: Compara√ß√£o (1 dia)
- Comparar outputs de qualidade
- Medir lat√™ncia real nos workflows
- Avaliar uso de mem√≥ria
- Documentar diferen√ßas comportamentais

### Fase 4: Decis√£o (1 dia)
- Escolher modelo final (provavelmente Qwen 3:14b ou 3:8b)
- Atualizar `crew_paraty.py` com novo default
- Atualizar documenta√ß√£o
- Commit das mudan√ßas

---

## üìö REFER√äNCIAS T√âCNICAS

### Berkeley Function Calling Leaderboard (BFCL)
- **Paper**: "The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation"
- **Autores**: Patil et al., 2025
- **Confer√™ncia**: Forty-second International Conference on Machine Learning
- **Cita√ß√£o**:
```bibtex
@inproceedings{patil2025bfcl,
  title={The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models},
  author={Patil, Shishir G. and Mao, Huanzhi and Cheng-Jie Ji, Charlie and Yan, Fanjia and Suresh, Vishnu and Stoica, Ion and E. Gonzalez, Joseph},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025},
}
```

### Docker Evaluation Study
- **T√≠tulo**: "Local LLM Tool Calling: Which LLM Should You Use?"
- **Data**: Junho 2025
- **Metodologia**: 21 modelos, 3,570 testes, hardware M4 Max
- **M√©tricas**: Tool Invocation, Tool Selection, Parameter Accuracy

### CrewAI Community Insights
- **Fonte**: https://community.crewai.com/
- **Destaque**: Qwen 2.5 e Qwen 3 s√£o mais recomendados pela comunidade
- **Confirma√ß√£o**: Function calling funciona bem com Qwen family

---

## üí° CONCLUS√ÉO

Com base na pesquisa extensiva de m√∫ltiplas fontes confi√°veis:

### üéØ VENCEDOR ABSOLUTO: **Qwen 3 (14B)**
- **F1 Score**: 0.971 (praticamente GPT-4 local!)
- **Melhor modelo local** para tool calling em 2025
- **Evolu√ß√£o natural** do nosso Qwen 2.5:14b
- **Migra√ß√£o de baixo risco** (mesma fam√≠lia)

### ü•à VICE-CAMPE√ÉO: **Qwen 3 (8B)**
- **Melhor custo-benef√≠cio** (0.933 F1, metade da lat√™ncia)
- **Ideal para desenvolvimento** iterativo
- **Hardware acess√≠vel** (8GB RAM)

### üèÜ NOSSA ESTRAT√âGIA:
1. ‚úÖ **Agora**: Manter Qwen 2.5:14b (estabilidade)
2. üß™ **Pr√≥xima semana**: Testar Qwen 3:8b (desenvolvimento)
3. üöÄ **Pr√≥ximo m√™s**: Migrar para Qwen 3:14b (produ√ß√£o)

### ‚ö†Ô∏è EVITAR:
- ‚ùå gpt-oss (incompat√≠vel - j√° documentado)
- ‚ùå xLAM-2-8B (inst√°vel)
- ‚ùå watt-tool-8B (baixa performance)

---

**Documento gerado em**: 31/10/2025  
**Pr√≥xima revis√£o**: Dezembro 2025 (novos modelos podem surgir)  
**Respons√°vel**: Equipe CrewAI Local
