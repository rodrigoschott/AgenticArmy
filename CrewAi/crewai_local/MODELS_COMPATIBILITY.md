# Compatibilidade de Modelos com CrewAI

**âš ï¸ ATUALIZAÃ‡ÃƒO IMPORTANTE (31/10/2025):**  
Consulte `RECOMMENDED_MODELS_RESEARCH.md` para pesquisa completa sobre os melhores modelos de 2025.

---

## ğŸ† TOP TIER: ALTAMENTE RECOMENDADOS (2025)

### Qwen 3 (14B) â­â­â­â­â­ **MELHOR MODELO LOCAL 2025**
- **Tamanho:** 14B parÃ¢metros (~9GB quantizado)
- **F1 Score:** 0.971 (praticamente GPT-4!)
- **Context:** 128k tokens
- **Pontos fortes:**
  - ğŸ† **Melhor modelo local** para tool calling segundo Docker evaluation
  - ğŸ¯ AcurÃ¡cia excepcional (96% em schema understanding)
  - ğŸŒ Multilingual - excelente portuguÃªs
  - ğŸ§  Reasoning avanÃ§ado - superior ao Qwen 2.5
  - âœ… CompatÃ­vel com CrewAI (mesma famÃ­lia Qwen)
- **LatÃªncia:** ~120-142s (aceitÃ¡vel para workflows complexos)
- **RAM NecessÃ¡ria:** 16GB+
- **Status:** âœ… Validado pela comunidade CrewAI
- **Comando:** 
  ```bash
  ollama pull qwen3:14b
  # Ou versÃ£o quantizada (menor RAM):
  ollama pull qwen3:14b-q4_k_m
  ```

### Qwen 3 (8B) â­â­â­â­â­ **MELHOR CUSTO-BENEFÃCIO**
- **Tamanho:** 8B parÃ¢metros (~5GB quantizado)
- **F1 Score:** 0.933 (empata com Claude 3 Haiku!)
- **Context:** 128k tokens
- **Pontos fortes:**
  - âš¡ **50% mais rÃ¡pido** que o 14B
  - ğŸ’° **Hardware acessÃ­vel** (8GB RAM)
  - ğŸ¯ F1 Score excelente (0.933)
  - ğŸŒ Mesmo suporte multilingual do 14B
  - âœ… Ideal para desenvolvimento iterativo
- **LatÃªncia:** ~70-84s
- **RAM NecessÃ¡ria:** 8GB+
- **Status:** âœ… Recomendado para dev/test
- **Comando:** 
  ```bash
  ollama pull qwen3:8b
  # Ou versÃ£o quantizada:
  ollama pull qwen3:8b-q4_k_m
  ```

### Qwen 2.5 (14B) â­â­â­â­ **JÃ VALIDADO NO PROJETO**
- **Tamanho:** 9.0 GB
- **F1 Score:** 0.812 (sÃ³lido)
- **Context:** 128k tokens
- **Pontos fortes:**
  - âœ… **JÃ¡ testado e validado** no nosso projeto
  - ğŸ¯ Excelente tool calling
  - ğŸ“ Ã“timo com estruturas complexas
  - ğŸ“‹ Segue templates rigorosamente
  - âš¡ RÃ¡pido em respostas
- **Status:** âœ… Testado e validado (100% success rate)
- **Comando:** `ollama pull qwen2.5:14b`
- **ğŸ’¡ Nota:** Qwen 3 oferece +16% de acurÃ¡cia. MigraÃ§Ã£o recomendada quando possÃ­vel.

---

## ğŸ–ï¸ TIER A: ALTERNATIVAS SÃ“LIDAS

### Llama 3.1 (8B Instruct) â­â­â­â­
- **Tamanho:** 8B parÃ¢metros
- **F1 Score:** 0.835
- **Context:** 128k tokens
- **Pontos fortes:**
  - ğŸ¢ Suporte Meta oficial
  - ğŸ“š Amplamente testado pela comunidade
  - ğŸ”§ Boa documentaÃ§Ã£o e exemplos
  - âœ… ConfiÃ¡vel para tool calling
- **LatÃªncia:** ~90s
- **RAM NecessÃ¡ria:** 8GB+
- **Status:** âœ… Fallback confiÃ¡vel
- **Comando:** `ollama pull llama3.1:8b-instruct`

### GLM-4.6 Cloud â­â­â­â­
- **Tamanho:** VariÃ¡vel (cloud)
- **Context:** Longo
- **Pontos fortes:**
  - â˜ï¸ Modelo em nuvem (sem uso local de recursos)
  - ğŸ¯ Excelente performance
  - ğŸ“Š Boa qualidade de anÃ¡lise
  - ğŸ”§ Suporte a ferramentas
- **Status:** âœ… Funcional
- **Comando:** `ollama pull glm-4.6:cloud`

### Llama 3.2 Latest â­â­â­
- **Tamanho:** 2.0 GB
- **F1 Score:** 0.727
- **Context:** 128k tokens
- **Pontos fortes:**
  - âš¡ Muito rÃ¡pido
  - ğŸ’¾ Eficiente em memÃ³ria
  - âœ… Bom para tasks simples
- **Status:** âœ… Funcional para testes rÃ¡pidos
- **Comando:** `ollama pull llama3.2:latest`

### Mistral 7B Instruct â­â­â­â­
- **Tamanho:** 7B parÃ¢metros
- **F1 Score:** 0.85-0.86 (estimado)
- **Pontos fortes:**
  - âš¡ **Muito eficiente** - menor uso de recursos
  - ğŸš€ **RÃ¡pido** - Ã³tima latÃªncia (~80s)
  - ğŸŒ **Multilingual** - bom suporte europeu
  - ğŸ“ **JSON schema adherence** excelente
- **RAM NecessÃ¡ria:** 7GB+
- **Status:** âœ… Ideal para hardware limitado
- **Comando:** `ollama pull mistral:7b-instruct`

---

## ğŸ”¬ MODELOS ESPECIALIZADOS

### Llama 3.1 (70B Instruct) â­â­â­â­â­ **POWERHOUSE**
- **Tamanho:** 70B parÃ¢metros (~40GB)
- **F1 Score:** ~0.94-0.96 (prÃ³ximo ao GPT-4)
- **Pontos fortes:**
  - ğŸ† AcurÃ¡cia excepcional
  - ğŸ§  Complex multi-step reasoning
  - ğŸ›¡ï¸ Error handling superior
  - ğŸ“š Context understanding avanÃ§ado
- **LatÃªncia:** ~240s (muito lento)
- **RAM NecessÃ¡ria:** 64GB+ recomendado
- **Status:** âš ï¸ Apenas para hardware potente
- **Comando:** `ollama pull llama3.1:70b-instruct`

### CodeLlama 13B Instruct â­â­â­
- **Tamanho:** 13B parÃ¢metros
- **F1 Score:** 0.88
- **EspecializaÃ§Ã£o:** Code generation e debugging
- **Pontos fortes:**
  - ğŸ’» Especializado em cÃ³digo
  - ğŸ“ API documentation - entende schemas tÃ©cnicos
  - ğŸ”§ Code generation superior
- **Status:** âœ… Ideal para workflows DevOps
- **Comando:** `ollama pull codellama:13b-instruct`

### Mixtral 8x7B Instruct â­â­â­â­
- **Tamanho:** 8Ã—7B parÃ¢metros (MoE) (~24GB)
- **F1 Score:** 0.88
- **Arquitetura:** Mixture of Experts
- **Pontos fortes:**
  - ğŸ¯ Versatilidade - experts para diferentes domÃ­nios
  - ğŸŒ Multilingual excelente
  - ğŸ“ Complex schemas - roteamento inteligente
- **RAM NecessÃ¡ria:** 24GB+
- **Status:** âœ… Ideal para multi-domÃ­nio
- **Comando:** `ollama pull mixtral:8x7b-instruct`

---

## âš ï¸ Modelos com LimitaÃ§Ãµes

### GPT-OSS âŒ **NÃƒO RECOMENDADO PARA CREWAI**
- **Tamanho:** 13 GB
- **Context:** 131k tokens
- **Problema:**
  ```
  ValueError: Invalid response from LLM call - None or empty.
  ```
  
**Por que nÃ£o funciona com CrewAI?**

O `gpt-oss` usa um **sistema de canais multi-stream** com trÃªs canais separados:
- **analysis** - RaciocÃ­nio interno (thinking)
- **commentary** - Tool calls
- **final** - Resposta final

**Comportamento:**
- âœ… Prompts simples: Usa apenas canal "final" â†’ **Funciona**
- âŒ Tool calls/contexto complexo: Ativa canal "analysis" primeiro â†’ **Falha**

Quando o modelo ativa o canal "analysis" (comum em workflows com ferramentas), ele retorna:
```
<|start|>assistant<|channel|>analysis<|message|>
[RaciocÃ­nio interno]
<|end|>
<|start|>assistant<|channel|>final<|message|>
[Resposta]
<|end|>
```

**Impacto:**
- CrewAI espera resposta direta no formato padrÃ£o
- O parser nÃ£o reconhece tags `<|channel|>analysis`
- Agentes falham com `ValueError: Invalid response from LLM call` em workflows complexos
- Funciona em testes simples mas falha em produÃ§Ã£o

**Quando usar gpt-oss:**
- âœ… Conversas standalone via `ollama run gpt-oss`
- âœ… Scripts Python simples sem ferramentas
- âŒ **NUNCA** com CrewAI workflows
- âŒ **NUNCA** com agents que usam tools

**Alternativa:**
Se vocÃª precisa de um modelo de 13GB+ com tool calling, use:
- `ollama pull deepseek-coder:14b` ou
- `ollama pull mixtral:8x7b`

### DeepSeek-Coder 33B âš ï¸ **USO ESPECÃFICO**
- **Tamanho:** 16 GB (quantizado Q3_K_M)
- **Context:** Longo
- **Pontos fortes:**
  - Excelente para cÃ³digo
  - Ã“timo para anÃ¡lise tÃ©cnica
- **LimitaÃ§Ãµes:**
  - Focado em cÃ³digo (nÃ£o em anÃ¡lise de negÃ³cio)
  - Requer muita RAM (16GB modelo + overhead)
- **Status:** âš ï¸ Funcional mas nÃ£o ideal para este projeto
- **Uso recomendado:** AnÃ¡lise tÃ©cnica de cÃ³digo, nÃ£o estratÃ©gia de negÃ³cio

---

## ğŸ”§ Testando Compatibilidade

### Teste RÃ¡pido (Terminal):
```bash
# Testar resposta bÃ¡sica
ollama run <modelo> "Hello, respond with 'OK'"

# Se responder "OK" sem prefixos especiais â†’ âœ… CompatÃ­vel
# Se responder "Thinking... OK" â†’ âŒ IncompatÃ­vel com CrewAI
```

### Teste Completo (Python):
```python
from crewai import LLM as CrewLLM

llm = CrewLLM(model="ollama/<modelo>", base_url="http://localhost:11434")

try:
    response = llm.call("Say hello")
    print(f"âœ… Compatible: {response}")
except Exception as e:
    print(f"âŒ Incompatible: {e}")
```

---

## ğŸ“Š Tabela de Compatibilidade

| Modelo | Tamanho | Tool Calling | CrewAI | Velocidade | Qualidade |
|--------|---------|--------------|--------|------------|-----------|
| **qwen2.5:14b** | 9 GB | â­â­â­â­â­ | âœ… | â­â­â­â­ | â­â­â­â­â­ |
| **glm-4.6:cloud** | ~ | â­â­â­â­â­ | âœ… | â­â­â­â­â­ | â­â­â­â­â­ |
| **llama3.2:latest** | 2 GB | â­â­â­ | âœ… | â­â­â­â­â­ | â­â­â­ |
| gpt-oss:latest | 13 GB | â­â­â­â­ | âŒ | â­â­â­ | â­â­â­â­ |
| deepseek-coder:33b | 16 GB | â­â­â­ | âš ï¸ | â­â­ | â­â­â­â­ |

---

## ğŸš€ RecomendaÃ§Ã£o por Caso de Uso

### Workflow Planejamento 30 Dias (Workflow D):
**Melhor opÃ§Ã£o:** `qwen2.5:14b`
- Precisa de tool calling forte
- Templates complexos
- AnÃ¡lise estratÃ©gica

### Workflow AvaliaÃ§Ã£o Propriedade (Workflow A):
**Melhor opÃ§Ã£o:** `qwen2.5:14b` ou `glm-4.6:cloud`
- AnÃ¡lise financeira detalhada
- MÃºltiplas ferramentas (maps, search, etc.)

### Workflow Posicionamento (Workflow B):
**Melhor opÃ§Ã£o:** `llama3.2:latest`
- Menos ferramentas
- Mais criatividade
- Velocidade importante

### Workflow Abertura (Workflow C):
**Melhor opÃ§Ã£o:** `qwen2.5:14b`
- Muitas ferramentas tÃ©cnicas
- PrecisÃ£o crÃ­tica

---

## ğŸ“‹ GUIA DE SELEÃ‡ÃƒO DE MODELOS

### ğŸ¯ Para Desenvolvimento e Testes
1. **qwen3:8b-q4_k_m** â­â­â­â­â­ (MELHOR)
   - F1: 0.933 | LatÃªncia: ~70s | RAM: 8GB
   - MigraÃ§Ã£o natural do Qwen 2.5

2. **qwen2.5:14b** â­â­â­â­ (JÃ VALIDADO)
   - F1: 0.812 | Continuar para estabilidade
   - JÃ¡ testado em todos os workflows

3. **llama3.2:latest** â­â­â­ (RÃPIDO)
   - F1: 0.727 | Use para testes rÃ¡pidos

### ğŸš€ Para ProduÃ§Ã£o
1. **qwen3:14b-q4_k_m** â­â­â­â­â­ (MELHOR LOCAL 2025)
   - F1: 0.971 (praticamente GPT-4!)
   - +16% acurÃ¡cia vs Qwen 2.5
   - Melhor reasoning e multilingual

2. **qwen2.5:14b** â­â­â­â­ (MANTER POR ENQUANTO)
   - JÃ¡ validado e estÃ¡vel
   - Migrar para Qwen 3 quando validado

3. **llama3.1:8b-instruct** â­â­â­â­ (FALLBACK)
   - F1: 0.835 | Suporte Meta oficial
   - Alternativa confiÃ¡vel

### ğŸ’» Para Hardware Limitado (<8GB RAM)
- **qwen3:8b-q4_k_m** (5GB) - Melhor opÃ§Ã£o
- **mistral:7b-instruct** (4GB) - Muito rÃ¡pido
- **llama3.2:latest** (2GB) - Testes bÃ¡sicos

### ğŸ† Para MÃ¡xima AcurÃ¡cia (Hardware Potente)
- **qwen3:14b** (16GB+) - F1: 0.971
- **llama3.1:70b-instruct** (64GB+) - F1: ~0.95
  - âš ï¸ LatÃªncia alta (~240s)

### ğŸ”§ Para Workflows DevOps/CÃ³digo
- **codellama:13b-instruct** - F1: 0.88
  - Especializado em code generation

### ğŸŒ Para AplicaÃ§Ãµes Multi-domÃ­nio
- **mixtral:8x7b-instruct** - F1: 0.88
  - Mixture of Experts

---

## ğŸ¯ NOSSA ESTRATÃ‰GIA DE MIGRAÃ‡ÃƒO

### âœ… Curto Prazo (PrÃ³ximas 2 semanas)
- **Manter Qwen 2.5:14b** para estabilidade
- JÃ¡ validado, funciona perfeitamente
- F1 Score 0.812 Ã© sÃ³lido

### ğŸ§ª MÃ©dio Prazo (1 mÃªs)
- **Testar Qwen 3:8b** em ambiente de dev
- **Avaliar Qwen 3:14b** em workflows reais
- Ganho de 12-16% em acurÃ¡cia esperado

### ğŸš€ Longo Prazo (2-3 meses)
- **Migrar para Qwen 3:14b** em produÃ§Ã£o
- Documentar comportamentos diferentes
- Manter Llama 3.1:8b como fallback

---

## âš™ï¸ CONFIGURAÃ‡ÃƒO RECOMENDADA

### .env
```bash
# Modelo padrÃ£o (manter estÃ¡vel por enquanto)
DEFAULT_MODEL=qwen2.5:14b

# Futuro: apÃ³s testes
# DEFAULT_MODEL=qwen3:14b-q4_k_m

# Ollama endpoint
OLLAMA_BASE_URL=http://localhost:11434
```

### InstalaÃ§Ã£o dos Modelos Recomendados:
```bash
# 1. ATUAL - Manter por estabilidade (9GB)
ollama pull qwen2.5:14b

# 2. PRÃ“XIMO - Testar para migraÃ§Ã£o (9GB)
ollama pull qwen3:14b-q4_k_m

# 3. ALTERNATIVA - Desenvolvimento rÃ¡pido (5GB)
ollama pull qwen3:8b-q4_k_m

# 4. FALLBACK - Backup confiÃ¡vel (5GB)
ollama pull llama3.1:8b-instruct

# 5. RÃPIDO - Testes bÃ¡sicos (2GB)
ollama pull llama3.2:latest
```

---

## ğŸ› Troubleshooting

### Erro: "Invalid response from LLM call - None or empty"
**Causa:** Modelo usando formato incompatÃ­vel (ex: gpt-oss)
**SoluÃ§Ã£o:** Use `qwen2.5:14b`, `glm-4.6:cloud` ou `llama3.2:latest`

### Erro: "Model not found"
**Causa:** Modelo nÃ£o instalado no Ollama
**SoluÃ§Ã£o:** 
```bash
ollama list  # Ver modelos instalados
ollama pull <modelo>  # Instalar modelo
```

### Erro: "Connection refused"
**Causa:** Ollama nÃ£o estÃ¡ rodando
**SoluÃ§Ã£o:**
```bash
# Windows/Mac: Iniciar Ollama app
# Linux: 
ollama serve
```

---

**Ãšltima atualizaÃ§Ã£o:** 2025-10-31  
**VersÃ£o:** 3.0  
**Pesquisa completa:** Veja `RECOMMENDED_MODELS_RESEARCH.md`

**Principais mudanÃ§as nesta versÃ£o:**
- âœ… Adicionados modelos Qwen 3 (8B e 14B) - melhores de 2025
- ğŸ“Š F1 Scores baseados em Docker evaluation e Berkeley BFCL
- ğŸ¯ Guia de seleÃ§Ã£o por caso de uso
- ğŸ“ˆ EstratÃ©gia de migraÃ§Ã£o definida
- ğŸ”— ReferÃªncias a pesquisas atualizadas
